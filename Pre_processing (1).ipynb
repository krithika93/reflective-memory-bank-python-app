{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FX78qSQlU8Th"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud1 init"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNHcbEbMWMcd",
        "outputId": "120a8508-02cc-4413-a7db-88e6d63b789f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: gcloud1: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil cp gs://notes_reflection/output10/part-00000 ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNO_5oetYEpo",
        "outputId": "edaa11eb-85d1-44ea-ffc9-9e971c6878c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://notes_reflection/output10/part-00000...\n",
            "/ [0 files][    0.0 B/336.4 KiB]                                                \r/ [1 files][336.4 KiB/336.4 KiB]                                                \r\n",
            "Operation completed over 1 objects/336.4 KiB.                                    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil cp gs://notes_reflection/output10/part-00001 ."
      ],
      "metadata": {
        "id": "tfZkGp0vgsxV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdb19f98-a8e7-463d-9bd5-8a799b46f874"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://notes_reflection/output10/part-00001...\n",
            "/ [0 files][    0.0 B/300.9 KiB]                                                \r/ [1 files][300.9 KiB/300.9 KiB]                                                \r\n",
            "Operation completed over 1 objects/300.9 KiB.                                    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "l = []\n",
        "with open(\"part-00000\",\"r\") as file:\n",
        "   data = file.read().strip()\n",
        "\n",
        "import re\n",
        "\n",
        "data=data.replace('{html}',\"\") \n",
        "data=re.sub(r'http\\S+', '',data)\n",
        "p = re.compile(r'[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}',re.MULTILINE)\n",
        "data  =p.sub(\"    \",data)\n",
        "p = re.compile(r'Null',re.MULTILINE)\n",
        "data  =p.sub(\"\",data)\n",
        "text = re.sub(r\"[^a-zA-Z0-9 \\n]\", \"\", data)\n",
        "text = re.sub(r\"1 3322423870\", \"\", text)\n",
        "r = text.split(\"\\n\")\n",
        "res= []\n",
        "for t in r:\n",
        "    t= t.strip()\n",
        "    if t !='':\n",
        "      res.append(t)\n",
        "print(len(res))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rj-r78Kzdd21",
        "outputId": "61efd3c5-baae-476a-8735-cbb2ff473b22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6982\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "l = []\n",
        "with open(\"part-00001\",\"r\") as file:\n",
        "   data = file.read().strip()\n",
        "\n",
        "import re\n",
        "\n",
        "data=data.replace('{html}',\"\") \n",
        "data=re.sub(r'http\\S+', '',data)\n",
        "p = re.compile(r'[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}',re.MULTILINE)\n",
        "data  =p.sub(\"    \",data)\n",
        "p = re.compile(r'Null',re.MULTILINE)\n",
        "p1 = re.compile(r'New Note',re.MULTILINE)\n",
        "data  =p.sub(\"\",data)\n",
        "data  =p1.sub(\"\",data)\n",
        "text = re.sub(r\"[^a-zA-Z0-9 \\n]\", \"\", data)\n",
        "\n",
        "r = text.split(\"\\n\")\n",
        "res1= []\n",
        "for t in r:\n",
        "    t= t.strip()\n",
        "    if t !='':\n",
        "      res1.append(t)\n",
        "print(len(res1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8V-VfME9NZZ8",
        "outputId": "a584282d-7510-4481-ee67-b401c61191cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6280\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Text preprocessiong\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "wn = nltk.WordNetLemmatizer()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9cKJpGwMka5",
        "outputId": "f2ee786d-a791-495b-cf7e-8063e3ae7a72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fin_res = res+res1\n",
        "df = pd.DataFrame(fin_res,columns=['content'])\n",
        "new_stopwords = [\"all\", \"due\", \"to\", \"on\"]\n",
        "\n",
        "# Remove stopwords\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "stopwords.extend(new_stopwords)\n",
        "print(f'There are {len(stopwords)} default stopwords. They are {stopwords}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPVf0EaiNpIr",
        "outputId": "b88abc96-7702-44f7-a452-f83029dec9e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 183 default stopwords. They are ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'all', 'due', 'to', 'on']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install textblob\n",
        "import spacy\n",
        "\n",
        "# Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
        "# Remove stopwords\n",
        "df['content1'] = df['content'].apply(lambda x: \" \".join(token.lemma_ for token in nlp(x)))\n",
        "\n",
        "print(df['content1'])\n",
        "print(df['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnlCiCdMkorL",
        "outputId": "f690adbf-01cb-47b5-b6ab-05c1f3f6b5e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0                                            fire in belly\n",
            "1         dont do chant with the AIM to remvove NEGATIVITY\n",
            "2        out of 1000 instance of Krishna I have some vi...\n",
            "3                                       we be do wonderful\n",
            "4               do not do offense of chant follow the rule\n",
            "                               ...                        \n",
            "13257    I feel happy that I stand even five minute eve...\n",
            "13258    very clear hard work definitely pay and we jus...\n",
            "13259                          supportindiaustraveldocscom\n",
            "13260    that two breath push down feel     I can not d...\n",
            "13261                                          toefletsorg\n",
            "Name: content1, Length: 13262, dtype: object\n",
            "0                                            Fire in belly\n",
            "1        DONT DO CHANTING WITH THE AIM TO REMVOVE NEGAT...\n",
            "2        Out of 1000 instances of Krishna i have some v...\n",
            "3                                   We are doing wonderful\n",
            "4             Dont do offense of chanting follow the rules\n",
            "                               ...                        \n",
            "13257    I feel happy that I stood even five minutes Ev...\n",
            "13258    very clear hard work definitely pays and we ju...\n",
            "13259                          supportindiaustraveldocscom\n",
            "13260    that two breaths Pushing down feeling    I can...\n",
            "13261                                          toefletsorg\n",
            "Name: content, Length: 13262, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df1 = df[df[\"content1\"].apply(lambda x: x.isnumeric()==False)]\n",
        "df2 = df1[df1[\"content1\"].apply(lambda x: len(x ) > 2 )]\n",
        "df2 = df2[~df2['content1'].astype(str).str.contains(\"fakh6\")]\n",
        "print(len(df2))\n",
        "df2[\"content1\"].to_csv(\"results3.csv\")\n"
      ],
      "metadata": {
        "id": "XA76X-5-yxh-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f56479fb-339b-4d8d-8ffb-491a70cdf0d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \n",
        "                       \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n",
        "                       \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \n",
        "                       \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\",\n",
        "                       \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \n",
        "                       \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\n",
        "                       \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\n",
        "                       \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \n",
        "                       \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
        "                       \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \n",
        "                       \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\n",
        "                       \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\",\n",
        "                       \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\",\n",
        "                       \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\",\n",
        "                       \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\",\n",
        "                       \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \n",
        "                       \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\",\n",
        "                       \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \n",
        "                       \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \n",
        "                       \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
        "                       \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\",\n",
        "                       \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", 'u.s':'america', 'e.g':'for example'}\n",
        "\n",
        "punct = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
        " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
        " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
        " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
        " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
        "\n",
        "punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\",\n",
        "                 \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', \n",
        "                 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', '!':' '}\n",
        "\n",
        "mispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater',\n",
        "                'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ',\n",
        "                'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can',\n",
        "                'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', \n",
        "                'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', \n",
        "                'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', \n",
        "                'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization',\n",
        "                'demonetisation': 'demonetization'}"
      ],
      "metadata": {
        "id": "QBX47DZf-LeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    '''Clean emoji, Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
        "    and remove words containing numbers.'''\n",
        "    text = emoji.demojize(text)\n",
        "    text = re.sub(r'\\:(.*?)\\:','',text)\n",
        "    text = str(text).lower()    #Making Text Lowercase\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    #The next 2 lines remove html text\n",
        "    text = BeautifulSoup(text, 'lxml').get_text()\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub('<.*?>+', '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\", \"'\")\n",
        "    text = re.sub(r\"[^a-zA-Z?.!,¿']+\", \" \", text)\n",
        "    return text\n",
        "\n",
        "def clean_contractions(text, mapping):\n",
        "    '''Clean contraction using contraction mapping'''    \n",
        "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
        "    for s in specials:\n",
        "        text = text.replace(s, \"'\")\n",
        "    for word in mapping.keys():\n",
        "        if \"\"+word+\"\" in text:\n",
        "            text = text.replace(\"\"+word+\"\", \"\"+mapping[word]+\"\")\n",
        "    #Remove Punctuations\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    # creating a space between a word and the punctuation following it\n",
        "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "    text = re.sub(r\"([?.!,¿])\", r\" \\1 \", text)\n",
        "    text = re.sub(r'[\" \"]+', \" \", text)\n",
        "    return text\n",
        "\n",
        "def clean_special_chars(text, punct, mapping):\n",
        "    '''Cleans special characters present(if any)'''   \n",
        "    for p in mapping:\n",
        "        text = text.replace(p, mapping[p])\n",
        "    \n",
        "    for p in punct:\n",
        "        text = text.replace(p, f' {p} ')\n",
        "    \n",
        "    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}  \n",
        "    for s in specials:\n",
        "        text = text.replace(s, specials[s])\n",
        "    \n",
        "    return text\n",
        "\n",
        "def correct_spelling(x, dic):\n",
        "    '''Corrects common spelling errors'''   \n",
        "    for word in dic.keys():\n",
        "        x = x.replace(word, dic[word])\n",
        "    return x\n",
        "\n",
        "def remove_space(text):\n",
        "    '''Removes awkward spaces'''   \n",
        "    #Removes awkward spaces \n",
        "    text = text.strip()\n",
        "    text = text.split()\n",
        "    return \" \".join(text)\n",
        "\n",
        "def text_preprocessing_pipeline(text):\n",
        "    '''Cleaning and parsing the text.'''\n",
        "    text = clean_text(text)\n",
        "    text = clean_contractions(text, contraction_mapping)\n",
        "    text = clean_special_chars(text, punct, punct_mapping)\n",
        "    text = correct_spelling(text, mispell_dict)\n",
        "    text = remove_space(text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "566i63g8-Mjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['content'] = df['content'] .apply(text_preprocessing_pipeline)"
      ],
      "metadata": {
        "id": "PIgYzyna_Suk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}