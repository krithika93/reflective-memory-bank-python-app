{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFCf-xl9Bnjt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5599dd59-44cc-42a5-e382-8d10078d0c16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting textattack\n",
            "  Downloading textattack-0.3.8-py3-none-any.whl (418 kB)\n",
            "\u001b[K     |████████████████████████████████| 418 kB 7.9 MB/s \n",
            "\u001b[?25hCollecting lru-dict\n",
            "  Downloading lru_dict-1.1.8-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
            "Requirement already satisfied: torch!=1.8,>=1.7.0 in /usr/local/lib/python3.8/dist-packages (from textattack) (1.13.0+cu116)\n",
            "Requirement already satisfied: click<8.1.0 in /usr/local/lib/python3.8/dist-packages (from textattack) (7.1.2)\n",
            "Collecting flair\n",
            "  Downloading flair-0.11.3-py3-none-any.whl (401 kB)\n",
            "\u001b[K     |████████████████████████████████| 401 kB 53.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.8/dist-packages (from textattack) (1.21.6)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.8/dist-packages (from textattack) (0.5.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from textattack) (1.7.1)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.8/dist-packages (from textattack) (0.42.1)\n",
            "Collecting word2number\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from textattack) (3.7)\n",
            "Collecting pycld2\n",
            "  Downloading pycld2-0.41.tar.gz (41.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 41.4 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from textattack) (4.64.1)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from textattack) (1.7.3)\n",
            "Collecting num2words\n",
            "  Downloading num2words-0.5.12-py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 57.3 MB/s \n",
            "\u001b[?25hCollecting pinyin==0.4.0\n",
            "  Downloading pinyin-0.4.0.tar.gz (3.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 58.6 MB/s \n",
            "\u001b[?25hCollecting transformers>=4.21.0\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 42.5 MB/s \n",
            "\u001b[?25hCollecting terminaltables\n",
            "  Downloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
            "Collecting lemminflect\n",
            "  Downloading lemminflect-0.2.3-py3-none-any.whl (769 kB)\n",
            "\u001b[K     |████████████████████████████████| 769 kB 62.3 MB/s \n",
            "\u001b[?25hCollecting language-tool-python\n",
            "  Downloading language_tool_python-2.7.1-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from textattack) (3.8.2)\n",
            "Collecting bert-score>=0.3.5\n",
            "  Downloading bert_score-0.3.12-py3-none-any.whl (60 kB)\n",
            "\u001b[K     |████████████████████████████████| 60 kB 6.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: more-itertools in /usr/local/lib/python3.8/dist-packages (from textattack) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from textattack) (1.3.5)\n",
            "Collecting datasets==2.4.0\n",
            "  Downloading datasets-2.4.0-py3-none-any.whl (365 kB)\n",
            "\u001b[K     |████████████████████████████████| 365 kB 59.9 MB/s \n",
            "\u001b[?25hCollecting OpenHowNet\n",
            "  Downloading OpenHowNet-2.0-py3-none-any.whl (18 kB)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 65.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets==2.4.0->textattack) (21.3)\n",
            "Collecting dill<0.3.6\n",
            "  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
            "\u001b[K     |████████████████████████████████| 95 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets==2.4.0->textattack) (9.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets==2.4.0->textattack) (3.8.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets==2.4.0->textattack) (2.23.0)\n",
            "Collecting huggingface-hub<1.0.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 63.0 MB/s \n",
            "\u001b[?25hCollecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 65.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets==2.4.0->textattack) (2022.11.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from bert-score>=0.3.5->textattack) (3.2.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.4.0->textattack) (6.0.3)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.4.0->textattack) (2.1.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.4.0->textattack) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.4.0->textattack) (1.8.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.4.0->textattack) (1.3.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.4.0->textattack) (4.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.4.0->textattack) (22.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==2.4.0->textattack) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==2.4.0->textattack) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets==2.4.0->textattack) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.0.1->textattack) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.0.1->textattack) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas>=1.0.1->textattack) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==2.4.0->textattack) (2022.12.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==2.4.0->textattack) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==2.4.0->textattack) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==2.4.0->textattack) (1.24.3)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 76.4 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 52.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.21.0->textattack) (2022.6.2)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.9 MB/s \n",
            "\u001b[?25hCollecting mpld3==0.3\n",
            "  Downloading mpld3-0.3.tar.gz (788 kB)\n",
            "\u001b[K     |████████████████████████████████| 788 kB 77.6 MB/s \n",
            "\u001b[?25hCollecting conllu>=4.0\n",
            "  Downloading conllu-4.5.2-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: gdown==4.4.0 in /usr/local/lib/python3.8/dist-packages (from flair->textattack) (4.4.0)\n",
            "Collecting janome\n",
            "  Downloading Janome-0.4.2-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.7 MB 71.5 MB/s \n",
            "\u001b[?25hCollecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[K     |████████████████████████████████| 981 kB 56.9 MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.95\n",
            "  Downloading sentencepiece-0.1.95-cp38-cp38-manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 65.5 MB/s \n",
            "\u001b[?25hCollecting segtok>=1.5.7\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from flair->textattack) (3.6.0)\n",
            "Collecting sqlitedict>=1.6.0\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from flair->textattack) (4.9.2)\n",
            "Collecting hyperopt>=0.2.7\n",
            "  Downloading hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 51.3 MB/s \n",
            "\u001b[?25hCollecting konoha<5.0.0,>=4.0.0\n",
            "  Downloading konoha-4.6.5-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from flair->textattack) (0.8.10)\n",
            "Collecting pptree\n",
            "  Downloading pptree-3.1.tar.gz (3.0 kB)\n",
            "Collecting wikipedia-api\n",
            "  Downloading Wikipedia_API-0.5.7-py3-none-any.whl (13 kB)\n",
            "Collecting deprecated>=1.2.4\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting bpemb>=0.3.2\n",
            "  Downloading bpemb-0.3.4-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.8/dist-packages (from flair->textattack) (1.0.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from gdown==4.4.0->flair->textattack) (4.6.3)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.8/dist-packages (from deprecated>=1.2.4->flair->textattack) (1.14.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.8/dist-packages (from gensim>=3.4.0->flair->textattack) (6.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from hyperopt>=0.2.7->flair->textattack) (0.16.0)\n",
            "Collecting py4j\n",
            "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
            "\u001b[K     |████████████████████████████████| 200 kB 63.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cloudpickle in /usr/local/lib/python3.8/dist-packages (from hyperopt>=0.2.7->flair->textattack) (1.5.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.8/dist-packages (from hyperopt>=0.2.7->flair->textattack) (2.8.8)\n",
            "Collecting importlib-metadata<4.0.0,>=3.7.0\n",
            "  Downloading importlib_metadata-3.10.1-py3-none-any.whl (14 kB)\n",
            "Collecting requests>=2.19.0\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.6 MB/s \n",
            "\u001b[?25hCollecting overrides<4.0.0,>=3.0.0\n",
            "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair->textattack) (3.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (0.11.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->flair->textattack) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->flair->textattack) (1.2.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy->flair->textattack) (0.2.5)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.13-py38-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 47.7 MB/s \n",
            "\u001b[?25hCollecting docopt>=0.6.2\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from OpenHowNet->textattack) (57.4.0)\n",
            "Collecting anytree\n",
            "  Downloading anytree-2.8.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 594 kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pinyin, mpld3, overrides, sqlitedict, langdetect, docopt, pptree, pycld2, word2number\n",
            "  Building wheel for pinyin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pinyin: filename=pinyin-0.4.0-py3-none-any.whl size=3630495 sha256=a6e6133bd47ad47e435945f0bff46783ac971c1f780a0f4c7387e5e2a48e318b\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/2a/d9/9c0f787a4d55f9a9eca26d322eafbe083bab41cb9bffb2e6e8\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-py3-none-any.whl size=116702 sha256=57ecb0e0e7b45252b3cb91e18e5f020098fd9f6e1d0a17124bf1917ab5e2eed0\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/9f/9d/d806a20bd97bc7076d724fa3e69fa5be61836ba16b2ffa6126\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10187 sha256=a3d56acaff57881d68eb86a273a46510d01419ac2b89d546d0bd66190adce227\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/4f/72/28857f75625b263e2e3f5ab2fc4416c0a85960ac6485007eaa\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16869 sha256=5ba1a1f4213da24c6aa9fc716e53acbc73fd7f751e450535ed5ccfa867935b79\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/c6/16/46e174009277f9bccdaa7215a243939d2f70180804b249bf3a\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=40c994c452ed1a364ec410e12f1da643d22b4a78c04654ae6ba7623c2beca196\n",
            "  Stored in directory: /root/.cache/pip/wheels/13/c7/b0/79f66658626032e78fc1a83103690ef6797d551cb22e56e734\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=7c428e04e6b14eab59c3f47d82ef6495c7b9683312344c4e80afc92d23d72fd8\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/ea/58/ead137b087d9e326852a851351d1debf4ada529b6ac0ec4e8c\n",
            "  Building wheel for pptree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4629 sha256=ddfd8d82fa172bdbff28d2bdb2cac33bcf72bb16fb2b8d05a9443ffd913496bd\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/8b/30/5b20240d3d13a9dfafb6a6dd49d1b541c86d39812cb3690edf\n",
            "  Building wheel for pycld2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycld2: filename=pycld2-0.41-cp38-cp38-linux_x86_64.whl size=9833557 sha256=373f22f855f717938936962eac69b6232871cf6b4e57e1a906a8f4a681c070f1\n",
            "  Stored in directory: /root/.cache/pip/wheels/2b/3a/82/d990040cbe6c3527732e931e2925785e83fe9aaa5a11c313ca\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5582 sha256=91a9fe5ed20cd1d6c876b7058501e50a06ad5e4bfb764fd4fd2b410d1998608e\n",
            "  Stored in directory: /root/.cache/pip/wheels/cb/f3/5a/d88198fdeb46781ddd7e7f2653061af83e7adb2a076d8886d6\n",
            "Successfully built pinyin mpld3 overrides sqlitedict langdetect docopt pptree pycld2 word2number\n",
            "Installing collected packages: urllib3, requests, tokenizers, sentencepiece, py4j, overrides, importlib-metadata, huggingface-hub, dill, xxhash, wikipedia-api, transformers, sqlitedict, segtok, responses, pptree, multiprocess, mpld3, langdetect, konoha, janome, hyperopt, ftfy, docopt, deprecated, conllu, bpemb, anytree, word2number, terminaltables, pycld2, pinyin, OpenHowNet, num2words, lru-dict, lemminflect, language-tool-python, flair, datasets, bert-score, textattack\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 5.1.0\n",
            "    Uninstalling importlib-metadata-5.1.0:\n",
            "      Successfully uninstalled importlib-metadata-5.1.0\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.6\n",
            "    Uninstalling dill-0.3.6:\n",
            "      Successfully uninstalled dill-0.3.6\n",
            "  Attempting uninstall: hyperopt\n",
            "    Found existing installation: hyperopt 0.1.2\n",
            "    Uninstalling hyperopt-0.1.2:\n",
            "      Successfully uninstalled hyperopt-0.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "markdown 3.4.1 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 3.10.1 which is incompatible.\n",
            "gym 0.25.2 requires importlib-metadata>=4.8.0; python_version < \"3.10\", but you have importlib-metadata 3.10.1 which is incompatible.\u001b[0m\n",
            "Successfully installed OpenHowNet-2.0 anytree-2.8.0 bert-score-0.3.12 bpemb-0.3.4 conllu-4.5.2 datasets-2.4.0 deprecated-1.2.13 dill-0.3.5.1 docopt-0.6.2 flair-0.11.3 ftfy-6.1.1 huggingface-hub-0.11.1 hyperopt-0.2.7 importlib-metadata-3.10.1 janome-0.4.2 konoha-4.6.5 langdetect-1.0.9 language-tool-python-2.7.1 lemminflect-0.2.3 lru-dict-1.1.8 mpld3-0.3 multiprocess-0.70.13 num2words-0.5.12 overrides-3.1.0 pinyin-0.4.0 pptree-3.1 py4j-0.10.9.7 pycld2-0.41 requests-2.28.1 responses-0.18.0 segtok-1.5.11 sentencepiece-0.1.95 sqlitedict-2.1.0 terminaltables-3.1.10 textattack-0.3.8 tokenizers-0.13.2 transformers-4.25.1 urllib3-1.25.11 wikipedia-api-0.5.7 word2number-1.1 xxhash-3.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dill",
                  "requests",
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install textattack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nM6Tq2ATBuMC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d9dc5ec-2f0f-4d93-dba4-658b559daf06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "textattack: Updating TextAttack package dependencies.\n",
            "textattack: Downloading NLTK required packages.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package omw to /root/nltk_data...\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        }
      ],
      "source": [
        "from textattack.augmentation import WordNetAugmenter\n",
        "from textattack.augmentation import EmbeddingAugmenter\n",
        "from textattack.augmentation import EasyDataAugmenter\n",
        "from textattack.augmentation import CharSwapAugmenter\n",
        "from textattack.augmentation import CheckListAugmenter\n",
        "from textattack.augmentation import CLAREAugmenter\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vm4vWpoOSoQ9",
        "outputId": "862377a3-90a3-445d-a1ca-2ccee00b8cec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "617\n",
            "806\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import csv\n",
        "import pandas as pd\n",
        "import re\n",
        " \n",
        "# Opening JSON file and loading the data\n",
        "# into the variable data\n",
        "with open('positive_memory_bank_annotations (6).json') as json_file:\n",
        "    data = json.load(json_file)\n",
        " \n",
        "employee_data = data['examples']\n",
        " \n",
        "# now we will open a file for writing\n",
        "data_file = open('data_file.csv', 'w')\n",
        " \n",
        "# create the csv writer object\n",
        "csv_writer = csv.writer(data_file)\n",
        " \n",
        "# Counter variable used for writing\n",
        "# headers to the CSV file\n",
        "count = 0\n",
        "\n",
        "# now we will open a file for writing\n",
        "data_file = open('data.csv', 'w')\n",
        " \n",
        "# create the csv writer object\n",
        "csv_writer = csv.writer(data_file)\n",
        " \n",
        "# Counter variable used for writing\n",
        "# headers to the CSV file\n",
        "count = 0\n",
        "df = pd.DataFrame(columns=[\"cont\",\"label\"])\n",
        "for emp in employee_data:\n",
        "              \n",
        "    if(len(emp['classifications']) > 0):\n",
        "      count += 1\n",
        "      class_name= \"\"\n",
        "      c=0\n",
        "      for j in emp['classifications']:\n",
        "           if c ==0:\n",
        "              class_name= class_name+ j['classname']\n",
        "           else:\n",
        "             class_name= class_name+\",\"+ j['classname']\n",
        "           c = c+1\n",
        "      sentence = re.sub('\\s+([^a-zA-Z]+)\\s+', ' \\\\1n ', str(emp['content']))\n",
        "      sentence = re.sub('[^\\s][^a-zA-Z]+\\s+', ' ', sentence)\n",
        "      # Single character removal\n",
        "      sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "      # Removing multiple spaces\n",
        "      sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "      sentence = sentence.lower()\n",
        "      df = df.append({\"cont\": sentence,\"label\": class_name},ignore_index= True)\n",
        "      \n",
        "     \n",
        "df.drop(df[df['label'] == \"Junk or Non-relevant\"].index, inplace=True)\n",
        "df.to_csv(\"res.csv\")\n",
        "print(len(df))\n",
        "print(count)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4Y44Ir-J3PV"
      },
      "outputs": [],
      "source": [
        "df.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4Fyfw3-CTdb",
        "outputId": "024d3fe4-8f4e-4b82-b404-fb62a6a9ae8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHpydPMCHbJb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5b0404a-2e5a-4f62-cd82-c9be999e29eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import nltk\n",
        "words = list(nltk.corpus.words.words())\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Python code to demonstrate working\n",
        "# of binary search in library\n",
        "from bisect import bisect_left\n",
        " \n",
        "def BinarySearch(a, x):\n",
        "    i = bisect_left(a, x)\n",
        "    if i != len(a) and a[i] == x:\n",
        "        return i\n",
        "    else:\n",
        "        return -1\n",
        "def textattack_data_augment(data, target, texattack_augmenter):\n",
        "\n",
        "  aug_data = []\n",
        "  aug_label = []\n",
        "  \n",
        "  for text, label in zip(data, target):\n",
        "    outer = False\n",
        "    for w in nltk.wordpunct_tokenize(text):\n",
        "      if  BinarySearch(words,w) == -1:\n",
        "        outer = True\n",
        "        break\n",
        "    if outer == True:\n",
        "      continue\n",
        "    text = \" \".join(w for w in nltk.wordpunct_tokenize(text) if w.lower() in words or not w.isalpha())\n",
        "    aug_list = texattack_augmenter.augment(text)\n",
        "    aug_data.append(text)\n",
        "    aug_label.append(label)\n",
        "    aug_data.extend(aug_list)\n",
        "    aug_label.extend([label]*len(aug_list))\n",
        "\n",
        "  return aug_data, aug_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3qlw4Y0Hlwi",
        "outputId": "9e2c0979-151b-488d-ddc7-f5afb0490f7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "textattack: Downloading https://textattack.s3.amazonaws.com/word_embeddings/paragramcf.\n",
            "100%|██████████| 481M/481M [00:12<00:00, 37.3MB/s]\n",
            "textattack: Unzipping file /root/.cache/textattack/tmpo6hebh0f.zip to /root/.cache/textattack/word_embeddings/paragramcf.\n",
            "textattack: Successfully saved word_embeddings/paragramcf to cache.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "617\n",
            "617\n"
          ]
        }
      ],
      "source": [
        "embed_aug = EmbeddingAugmenter(pct_words_to_swap=0.1, transformations_per_example=1)\n",
        "aug_data, aug_label = textattack_data_augment(list(df[\"cont\"]), list(df[\"label\"]), embed_aug)\n",
        "\n",
        "print(len(list(df['cont'])))\n",
        "print(len(list(df['label'])))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc7NqVcokSpY",
        "outputId": "cd72a1f9-7548-401b-ea86-0bd8cabf1cb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2                                                   idea\n",
            "8                                                  my ms\n",
            "9                                              awareness\n",
            "10                                         fire in belly\n",
            "12                                                 karna\n",
            "                             ...                        \n",
            "794    one of my guru in virginia who introduce to te...\n",
            "795    today in library read with different mindset t...\n",
            "799                                  in face agna chakra\n",
            "801                        we can activate agna together\n",
            "802         we can not afford to talk to nonsense people\n",
            "Name: cont, Length: 617, dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(df[\"cont\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBgRLhjJK4hd",
        "outputId": "1a89daa9-2732-4c71-9195-7415bdb9d97f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "685\n",
            "                                                  cont  \\\n",
            "0                                                 plan   \n",
            "1                                               scheme   \n",
            "2                                                 flow   \n",
            "3                                                 flux   \n",
            "4                                                 fame   \n",
            "..                                                 ...   \n",
            "680  one of my guru in virginia who introduce to te...   \n",
            "681  today in library read with different mindset t...   \n",
            "682                                in face agna chakra   \n",
            "683                      we can activate agna together   \n",
            "684       we can not afford to talk to nonsense people   \n",
            "\n",
            "                                                 label  \n",
            "0                                              Thought  \n",
            "1                                              Thought  \n",
            "2                               Value System,Principle  \n",
            "3                               Value System,Principle  \n",
            "4                                              Thought  \n",
            "..                                                 ...  \n",
            "680                         fact or intention,Spritual  \n",
            "681  example or result,positive intention or perspe...  \n",
            "682                                   Thought,Spritual  \n",
            "683         positive intention or perspective,Spritual  \n",
            "684                                       Value System  \n",
            "\n",
            "[685 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "df1 = pd.DataFrame(columns =[\"cont\",\"label\"])\n",
        "df1['cont'] = aug_data+list(df['cont'])\n",
        "df1['label'] = aug_label+list(df['label'])\n",
        "print(len(aug_data+list(df['cont'])))\n",
        "print(df1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1['label']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EV2qP7CW-MHP",
        "outputId": "2b4af251-f5fa-4b7a-fbc8-981486a87ec8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                                Thought\n",
              "1                                                Thought\n",
              "2                                 Value System,Principle\n",
              "3                                 Value System,Principle\n",
              "4                                                Thought\n",
              "                             ...                        \n",
              "680                           fact or intention,Spritual\n",
              "681    example or result,positive intention or perspe...\n",
              "682                                     Thought,Spritual\n",
              "683           positive intention or perspective,Spritual\n",
              "684                                         Value System\n",
              "Name: label, Length: 685, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QD5pAbqKMgQV",
        "outputId": "ec4dffb2-0240-4c47-e82f-a25134ba1032"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "\n",
        "embed_aug =  EasyDataAugmenter(pct_words_to_swap=0.1, transformations_per_example=1)\n",
        "aug_data1, aug_label1 = textattack_data_augment(list(df1[\"cont\"]), list(df1[\"label\"]), embed_aug)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtlFp-e-P9ee"
      },
      "outputs": [],
      "source": [
        "df3 = pd.DataFrame(columns =[\"cont\",\"label\"])\n",
        "df3['cont'] = aug_data1+list(df1['cont'])\n",
        "df3['label'] = aug_label1+list(df1['label'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4hks9HhOWHQ"
      },
      "outputs": [],
      "source": [
        "df3 = df3.replace('/', ',', regex=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NsgjasyN2yw",
        "outputId": "cb73e8a6-734f-40fe-d254-0493b1321a87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                  cont  \\\n",
            "0                                                 plan   \n",
            "1                                                 plan   \n",
            "2                                                 flow   \n",
            "3                                               period   \n",
            "4                                                 flux   \n",
            "..                                                 ...   \n",
            "836  one of my guru in virginia who introduce to te...   \n",
            "837  today in library read with different mindset t...   \n",
            "838                                in face agna chakra   \n",
            "839                      we can activate agna together   \n",
            "840       we can not afford to talk to nonsense people   \n",
            "\n",
            "                                                 label  \n",
            "0                                              Thought  \n",
            "1                                              Thought  \n",
            "2                               Value System,Principle  \n",
            "3                               Value System,Principle  \n",
            "4                               Value System,Principle  \n",
            "..                                                 ...  \n",
            "836                         fact or intention,Spritual  \n",
            "837  example or result,positive intention or perspe...  \n",
            "838                                   Thought,Spritual  \n",
            "839         positive intention or perspective,Spritual  \n",
            "840                                       Value System  \n",
            "\n",
            "[841 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "print(df3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df3.to_csv(\"result.csv\")"
      ],
      "metadata": {
        "id": "l8JZBC9UN_XM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aug_data1, aug_label1 = textattack_data_augment(list(df1[\"cont\"]), list(df1[\"label\"]), embed_aug)\n"
      ],
      "metadata": {
        "id": "bMu72bAZTWUx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}